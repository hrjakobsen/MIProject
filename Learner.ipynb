{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hexagon game Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of the hexagon game written in a functional (stateless) style so that we can simulate actions in our Q-learner without changing the actual game board.\n",
    "\n",
    "For storing the game board we use a numpy array instead of python lists, to make some of the operations less costly, as there is a lot of copying arrays going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def randomGame(width, height):\n",
    "    board = np.random.randint(5, size=(height + 1, width))\n",
    "    \n",
    "    # What we actually want is a jagged array where the odd cols \n",
    "    # are shifted half a cell up, to simulate a hexagon grid.\n",
    "    # We achieve this by making the grid 1 higher than specified and \n",
    "    # 'remove' the top cell in the odd columns\n",
    "    for x in range(width // 2):\n",
    "        board[0][x * 2 + 1] = -1\n",
    "        \n",
    "    # Set the initial player positions\n",
    "    board[0, 0] = 5\n",
    "    board[height, width - 1] = 10\n",
    "    \n",
    "    return board\n",
    "\n",
    "def getHash(board):\n",
    "    # This function generates a unique hash for each board\n",
    "    # This is done by realising that each cell on the board\n",
    "    # can be in one of 15 states (5 colors * 3 owned-states)\n",
    "    # This means that each cell can be stored in a single \n",
    "    # base-16 character\n",
    "       \n",
    "    return ''.join([format(int(i), 'x') if i != - 1 else \"\" for i in np.nditer(board.T)])\n",
    "\n",
    "def makeMove(board, player, action):\n",
    "    # Do everything on a copy to ensure stateless-ness\n",
    "    board = board.copy()\n",
    "    \n",
    "    height = board.shape[0]\n",
    "    width = board.shape[1]\n",
    "    \n",
    "    frontier = getOwnedCells(board, player)\n",
    "    \n",
    "    # Our color can spread through cells that were just added\n",
    "    # so we maintain a frontier that is the cells that we still\n",
    "    # need to check for neighbors of the right colour\n",
    "    while len(frontier):\n",
    "        point = frontier.pop()\n",
    "        board[point[0], point[1]] = action + player * 5\n",
    "        \n",
    "        neighbours = pointsAround(point)\n",
    "        \n",
    "        # Find the neighbours that are inside the board and\n",
    "        # have the color of the action and add them to the frontier \n",
    "        for neighbour in neighbours:\n",
    "            if (0 <= neighbour[0] < height \n",
    "                and 0 <= neighbour[1] < width \n",
    "                and board[neighbour[0], neighbour[1]] != -1 \n",
    "                and board[neighbour[0], neighbour[1]] == action):\n",
    "                frontier.append(neighbour)\n",
    "    \n",
    "    # If a player has no more moves, the other player is rewarded\n",
    "    # the rest of the cells on the board\n",
    "    board = finaliseBoard(board, player)\n",
    "    return board\n",
    "\n",
    "def getOwnedCells(board, player):\n",
    "    lowerLimit = player * 5\n",
    "    upperLimit = (player + 1) * 5\n",
    "    \n",
    "    frontier = list(\n",
    "        np.column_stack(\n",
    "            np.where(\n",
    "                np.logical_and(board < upperLimit, board >= lowerLimit)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return frontier\n",
    "\n",
    "def finaliseBoard(board, playerCall):\n",
    "    board = board.copy()\n",
    "    player = 2 if playerCall == 1 else 1\n",
    "    frontier = getOwnedCells(board, player)\n",
    "    height = board.shape[0]\n",
    "    width = board.shape[1]\n",
    "    \n",
    "    playerColor = board[0, 0] if playerCall == 1 else board[height - 1, width - 1]\n",
    "    found = False\n",
    "    \n",
    "    # Check if the player has a valid action that gains more cells\n",
    "    for cell in frontier:\n",
    "        neighbours = pointsAround(cell)\n",
    "        for neighbour in neighbours:\n",
    "            if (0 <= neighbour[0] < height \n",
    "                and 0 <= neighbour[1] < width \n",
    "                and 0 <= board[neighbour[0], neighbour[1]] < 5):\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "    if found:\n",
    "        return board\n",
    "    \n",
    "    # Award the player the rest of the cells\n",
    "    for x in np.nditer(board, op_flags=['readwrite']):\n",
    "        if 0 <= int(x) < 5:\n",
    "            x[...] = playerColor\n",
    "    return board\n",
    "\n",
    "def getReward(game, player):\n",
    "    # This function calculates the reward of a game\n",
    "    # We reward nothing for a move unless it is a winning move\n",
    "    # Then it gains 1 point. If it is a losing move, it gains -1\n",
    "    if not gameEnded(game):\n",
    "        return 0\n",
    "\n",
    "    height = game.shape[0]\n",
    "    width = game.shape[1]\n",
    "    \n",
    "    player1Color = game[0,0]\n",
    "    player2Color = game[height - 1, width - 1]\n",
    "    player1count = np.count_nonzero(game == player1Color)\n",
    "    player2count = np.count_nonzero(game == player2Color)\n",
    "    \n",
    "    if player == 1:\n",
    "        if player1count > player2count:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        if player2count > player1count:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "def gameEnded(board):\n",
    "    # Simply checks if there are any cells that are not owned \n",
    "    return not np.any(np.logical_and(board >= 0, board < 5))\n",
    "\n",
    "def pointsAround(point):\n",
    "    y, x = point[0], point[1]\n",
    "    odd = x % 2 == 1\n",
    "    neighbours = []\n",
    "    \n",
    "    relOddCoords = [\n",
    "        (-1, 0),\n",
    "        (-1, 1),\n",
    "        (0, 1),\n",
    "        (1, 0),\n",
    "        (0, -1),\n",
    "        (-1, -1)\n",
    "    ]\n",
    "\n",
    "    relEvenCoords = [\n",
    "        (-1, 0),\n",
    "        (0, 1),\n",
    "        (1, 1),\n",
    "        (1, 0),\n",
    "        (1, -1),\n",
    "        (0, -1)\n",
    "    ]\n",
    "    \n",
    "    for coord in relOddCoords if odd else relEvenCoords:\n",
    "        newY = y + coord[0]\n",
    "        newX = x + coord[1]\n",
    "        neighbours.append([newY, newX])\n",
    "    return neighbours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below is an implementaiton of the Q-Learning-Agent from Russel & Norvig (2010) that maintains a State-Action array $Q$ and uses it to learns the best actions from the reward $r$ of a given state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HexLearner(object):\n",
    "    # This class is an implementation of the Q-Learning-Agent\n",
    "    # from Russel & Norvig (2010) p. 844\n",
    "    \n",
    "    def __init__(self, player, Q = {}, N = {}):\n",
    "        self.Q = Q\n",
    "        self.N = N\n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.r = None\n",
    "        self.player = player\n",
    "        self.actions = [0, 1, 2, 3, 4]\n",
    "    \n",
    "    def getMove(self, state, reward):\n",
    "        if gameEnded(state):\n",
    "            self.Q[getHash(state), None] = reward\n",
    "        if self.s is not None:\n",
    "            self.incrementN()\n",
    "            self.updateQ(state, reward)\n",
    "        self.s = state\n",
    "        self.a = self.argmax(state)\n",
    "        self.r = reward\n",
    "        return self.a\n",
    "    \n",
    "    def initializeQ(self, s, a):\n",
    "        if (s, a) not in self.Q:\n",
    "            self.Q[s, a] = 0\n",
    "    \n",
    "    def initializeN(self, s, a):\n",
    "        if (s, a) not in self.N:\n",
    "            self.N[s, a] = 0\n",
    "    \n",
    "    def argmax(self, state):\n",
    "        vals = []\n",
    "        s = getHash(state)\n",
    "        for a in self.actions:\n",
    "            self.initializeQ(s, a)\n",
    "            self.initializeN(s, a)\n",
    "            vals.append(self.f(self.Q[s, a], self.N[s, a]))\n",
    "        return self.actions.index(vals.index(max(vals)))\n",
    "            \n",
    "    \n",
    "    def f(self, val, num):\n",
    "        if num < 1:\n",
    "            return 1\n",
    "        return val\n",
    "    \n",
    "    def incrementN(self):\n",
    "        s = getHash(self.s)\n",
    "        a = self.a\n",
    "        self.initializeN(s, a)\n",
    "        self.N[s, a] += 1\n",
    "    \n",
    "    def updateQ(self, sP, rP):\n",
    "        s = getHash(self.s)\n",
    "        a = self.a\n",
    "        self.initializeQ(s, a)\n",
    "        self.Q[s, a] = self.Q[s, a] + self.alpha() * (self.r + self.getBestQToSp(sP) - self.Q[s, a])\n",
    "        \n",
    "    def alpha(self):\n",
    "        return 1 / self.N[getHash(self.s), self.a]\n",
    "    \n",
    "    def getBestQToSp(self, sP):\n",
    "        newState = getHash(sP)\n",
    "        bestAction = None\n",
    "        bestQ = -1000000000000\n",
    "        for a in self.actions:\n",
    "            self.initializeQ(newState, a)\n",
    "            if self.Q[newState, a] > bestQ:\n",
    "                bestAction = a\n",
    "                bestQ = self.Q[newState, a]\n",
    "        return bestQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is ready to learn! The code below lets the agent play against it self and learn along the way. We use a value $\\varepsilon$ to denote the probability of choosing a random action instead of the action supplied by the agent. This is done to force the agent to explore more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def learn(trials, epsilon):\n",
    "    NUMTRIALS = trials\n",
    "    EPSILON = epsilon\n",
    "    latestQ1 = {}\n",
    "    latestN1 = {}\n",
    "    latestQ2 = {}\n",
    "    latestN2 = {}\n",
    "\n",
    "    startTime = time.time()\n",
    "    startGame = randomGame(3, 3)\n",
    "\n",
    "    for x in range(NUMTRIALS):\n",
    "        agent1 = HexLearner(1, latestQ1, latestN1)\n",
    "        agent2 = HexLearner(2, latestQ2, latestN2)\n",
    "        #game = randomGame(3,3)\n",
    "        game = startGame.copy()\n",
    "        while not gameEnded(game):\n",
    "            num = np.random.rand()\n",
    "            action = agent1.getMove(game, getReward(game, 1))\n",
    "            if num < EPSILON:\n",
    "                action = np.random.randint(5)\n",
    "            game = makeMove(game, 1, action)\n",
    "            \n",
    "            action = agent2.getMove(game, getReward(game, 2))\n",
    "            if num < EPSILON:\n",
    "                action = np.random.randint(5)\n",
    "            game = makeMove(game, 2, action)\n",
    "            \n",
    "        agent1.getMove(game, getReward(game, 1))\n",
    "        agent1.getMove(game, getReward(game, 1))\n",
    "        agent2.getMove(game, getReward(game, 2))\n",
    "        latestQ1 = agent1.Q\n",
    "        latestN1 = agent1.N\n",
    "        latestQ2 = agent2.Q\n",
    "        latestN2 = agent2.N\n",
    "\n",
    "        #if x % (NUMTRIALS / 10000) == 0 and x > 0:\n",
    "        #    elapsedSec = time.time() - startTime\n",
    "        #    gamesPerSec = x / elapsedSec\n",
    "        #    remainingSec = str(round((NUMTRIALS - x) / gamesPerSec, 2))\n",
    "        #    percentDone = str(round(x / NUMTRIALS * 100, 2))\n",
    "\n",
    "        #    clear_output(wait=True)\n",
    "        #    print(percentDone + \"% done - \" + remainingSec + \"s remaining\")\n",
    "\n",
    "    print(\"\\nDone! - Took \" + str(round(time.time() - startTime, 2)) + \"s. Saw \" + str(len(latestQ1)) + \" states\")\n",
    "    return latestQ1, latestN1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! - Took 8.13s. Saw 75252 states\n"
     ]
    }
   ],
   "source": [
    "latestQ1, latestN1 = learn(1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! - Took 3.53s. Saw 1786 states\n",
      "         3555991 function calls in 3.532 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    90126    0.900    0.000    1.172    0.000 <ipython-input-164-7c4ac6e69851>:27(<listcomp>)\n",
      "    16970    0.491    0.000    1.527    0.000 <ipython-input-164-7c4ac6e69851>:29(makeMove)\n",
      "   102335    0.415    0.000    0.449    0.000 <ipython-input-164-7c4ac6e69851>:135(pointsAround)\n",
      "   991386    0.272    0.000    0.272    0.000 {built-in method builtins.format}\n",
      "    33940    0.162    0.000    0.368    0.000 <ipython-input-164-7c4ac6e69851>:61(getOwnedCells)\n",
      "    16970    0.156    0.000    0.504    0.000 <ipython-input-164-7c4ac6e69851>:75(finaliseBoard)\n",
      "    45925    0.111    0.000    0.276    0.000 <ipython-input-164-7c4ac6e69851>:131(gameEnded)\n",
      "    18470    0.110    0.000    0.440    0.000 <ipython-input-153-6393e42ee1ae>:33(argmax)\n",
      "    90126    0.110    0.000    1.303    0.000 <ipython-input-164-7c4ac6e69851>:20(getHash)\n",
      "    45925    0.078    0.000    0.078    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "    33940    0.069    0.000    0.069    0.000 {built-in method numpy.core.multiarray.concatenate}\n",
      "    33940    0.066    0.000    0.176    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\numpy\\lib\\shape_base.py:314(column_stack)\n",
      "   181685    0.049    0.000    0.049    0.000 {built-in method numpy.core.multiarray.array}\n",
      "    34440    0.047    0.000    0.047    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
      "   778992    0.046    0.000    0.046    0.000 {method 'append' of 'list' objects}\n",
      "   197170    0.044    0.000    0.044    0.000 <ipython-input-153-6393e42ee1ae>:25(initializeQ)\n",
      "    17470    0.043    0.000    0.304    0.000 <ipython-input-153-6393e42ee1ae>:63(getBestQToSp)\n",
      "    45925    0.036    0.000    0.165    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1886(any)\n",
      "    17470    0.036    0.000    0.851    0.000 <ipython-input-153-6393e42ee1ae>:54(updateQ)\n",
      "    18470    0.033    0.000    1.754    0.000 <ipython-input-153-6393e42ee1ae>:14(getMove)\n",
      "        1    0.032    0.032    3.531    3.531 <ipython-input-165-d66edb9182f2>:4(learn)\n",
      "    33940    0.030    0.000    0.030    0.000 {built-in method numpy.core.multiarray.where}\n",
      "   109820    0.029    0.000    0.029    0.000 <ipython-input-153-6393e42ee1ae>:29(initializeN)\n",
      "    90126    0.022    0.000    0.022    0.000 {method 'join' of 'str' objects}\n",
      "    17470    0.021    0.000    0.307    0.000 <ipython-input-153-6393e42ee1ae>:48(incrementN)\n",
      "    45925    0.018    0.000    0.107    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
      "     8485    0.014    0.000    0.014    0.000 {method 'rand' of 'mtrand.RandomState' objects}\n",
      "    18470    0.013    0.000    0.133    0.000 <ipython-input-164-7c4ac6e69851>:105(getReward)\n",
      "    17470    0.013    0.000    0.257    0.000 <ipython-input-153-6393e42ee1ae>:60(alpha)\n",
      "    45925    0.010    0.000    0.088    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\numpy\\core\\_methods.py:37(_any)\n",
      "    68102    0.010    0.000    0.010    0.000 {method 'pop' of 'list' objects}\n",
      "    45925    0.010    0.000    0.023    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\numpy\\core\\numeric.py:534(asanyarray)\n",
      "    92350    0.009    0.000    0.009    0.000 <ipython-input-153-6393e42ee1ae>:43(f)\n",
      "    18470    0.007    0.000    0.007    0.000 {built-in method builtins.max}\n",
      "    36940    0.006    0.000    0.006    0.000 {method 'index' of 'list' objects}\n",
      "    85073    0.006    0.000    0.006    0.000 {built-in method builtins.len}\n",
      "     1665    0.005    0.000    0.005    0.000 {method 'randint' of 'mtrand.RandomState' objects}\n",
      "     3552    0.001    0.000    0.002    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\numpy\\core\\numeric.py:380(count_nonzero)\n",
      "     3552    0.001    0.000    0.001    0.000 {built-in method numpy.core.multiarray.count_nonzero}\n",
      "     1000    0.001    0.000    0.001    0.000 <ipython-input-153-6393e42ee1ae>:5(__init__)\n",
      "        1    0.000    0.000    3.532    3.532 <ipython-input-166-4cbeaa442aaa>:6(<module>)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel\\iostream.py:180(schedule)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "        2    0.000    0.000    3.532    1.766 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2880(run_code)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-164-7c4ac6e69851>:4(randomGame)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel\\iostream.py:342(write)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\codeop.py:132(__call__)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method nt.urandom}\n",
      "        3    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\threading.py:1104(is_alive)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\threading.py:1062(_wait_for_tstate_lock)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.round}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\IPython\\core\\hooks.py:142(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel\\iostream.py:284(_is_master_process)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel\\iostream.py:87(_event_pipe)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-166-4cbeaa442aaa>:7(<module>)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\IPython\\utils\\ipstruct.py:125(__getattr__)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel\\iostream.py:297(_schedule_flush)\n",
      "        2    0.000    0.000    3.532    1.766 {built-in method builtins.exec}\n",
      "        3    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\threading.py:506(is_set)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\IPython\\core\\interactiveshell.py:1069(user_global_ns)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\users\\henrik\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\IPython\\core\\hooks.py:207(pre_run_code_hook)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0xf23750>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile, pstats\n",
    "np.random.seed(0)\n",
    "\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "latestQ1, latestN1 = learn(500, 0.1)\n",
    "pr.disable()\n",
    "ps = pstats.Stats(pr).sort_stats('tottime')\n",
    "ps.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to play against the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = HexLearner(1, latestQ1, latestN1)\n",
    "game = startGame.copy()#randomGame(3, 3)\n",
    "print(game)\n",
    "\n",
    "while not gameEnded(game):\n",
    "    action = agent.getMove(game, getReward(game, 1))\n",
    "    if action != None:\n",
    "        print(\"P1: \" + str(action))\n",
    "        game = makeMove(game, 1, action)\n",
    "    print(game)\n",
    "    \n",
    "    game = makeMove(game, 2, int(input()))\n",
    "    print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(latestQ1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent1 = HexLearner(1, latestQ1, latestN1)\n",
    "agent2 = HexLearner(2, latestQ2, latestN2)\n",
    "game = startGame.copy()#randomGame(3, 3)\n",
    "print(game)\n",
    "\n",
    "while not gameEnded(game):\n",
    "    action1 = agent1.getMove(game, getReward(game, 1))\n",
    "    if action1 != None:\n",
    "        print(\"P1: \" + str(action1))\n",
    "        game = makeMove(game, 1, action1)\n",
    "    print(game)\n",
    "    \n",
    "    action2 = agent2.getMove(game, getReward(game, 2))\n",
    "    if action2 != None:\n",
    "        print(\"P2: \" + str(action2))\n",
    "        game = makeMove(game, 2, action2)\n",
    "    print(game)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
