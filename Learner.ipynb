{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hexagon game Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of the hexagon game written in a functional (stateless) style so that we can simulate actions in our Q-learner without changing the actual game board.\n",
    "\n",
    "For storing the game board we use a numpy array instead of python lists, to make some of the operations less costly, as there is a lot of copying arrays going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def randomGame(width, height):\n",
    "    board = np.random.randint(5, size=(height + 1, width))\n",
    "    \n",
    "    # What we actually want is a jagged array where the odd cols \n",
    "    # are shifted half a cell up, to simulate a hexagon grid.\n",
    "    # We achieve this by making the grid 1 higher than specified and \n",
    "    # 'remove' the top cell in the odd columns\n",
    "    for x in range(width // 2):\n",
    "        board[0][x * 2 + 1] = -1\n",
    "        \n",
    "    # Set the initial player positions\n",
    "    board[0, 0] = 5\n",
    "    board[height, width - 1] = 10\n",
    "    \n",
    "    return board\n",
    "\n",
    "def getHash(board):\n",
    "    # This function generates a unique hash for each board\n",
    "    # This is done by realising that each cell on the board\n",
    "    # can be in one of 15 states (5 colors * 3 owned-states)\n",
    "    # This means that each cell can be stored in a single \n",
    "    # base-16 character\n",
    "       \n",
    "    return ''.join([format(int(i), 'x') if i != - 1 else \"\" for i in np.nditer(board.T)])\n",
    "\n",
    "def makeMove(board, player, action):\n",
    "    # Do everything on a copy to ensure stateless-ness\n",
    "    board = board.copy()\n",
    "    \n",
    "    height = board.shape[0]\n",
    "    width = board.shape[1]\n",
    "    \n",
    "    frontier = getOwnedCells(board, player)\n",
    "    \n",
    "    # Our color can spread through cells that were just added\n",
    "    # so we maintain a frontier that is the cells that we still\n",
    "    # need to check for neighbors of the right colour\n",
    "    while len(frontier):\n",
    "        point = frontier.pop()\n",
    "        board[point[0], point[1]] = action + player * 5\n",
    "        \n",
    "        neighbours = pointsAround(point)\n",
    "        \n",
    "        # Find the neighbours that are inside the board and\n",
    "        # have the color of the action and add them to the frontier \n",
    "        for neighbour in neighbours:\n",
    "            if (0 <= neighbour[0] < height \n",
    "                and 0 <= neighbour[1] < width \n",
    "                and board[neighbour[0], neighbour[1]] != -1 \n",
    "                and board[neighbour[0], neighbour[1]] == action):\n",
    "                frontier.append(neighbour)\n",
    "    \n",
    "    # If a player has no more moves, the other player is rewarded\n",
    "    # the rest of the cells on the board\n",
    "    board = finaliseBoard(board, player)\n",
    "    return board\n",
    "\n",
    "def getOwnedCells(board, player):\n",
    "    lowerLimit = player * 5\n",
    "    upperLimit = (player + 1) * 5\n",
    "    \n",
    "    frontier = list(\n",
    "        np.column_stack(\n",
    "            np.where(\n",
    "                np.logical_and(board < upperLimit, board >= lowerLimit)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return frontier\n",
    "\n",
    "def finaliseBoard(board, playerCall):\n",
    "    board = board.copy()\n",
    "    player = 2 if playerCall == 1 else 1\n",
    "    frontier = getOwnedCells(board, player)\n",
    "    height = board.shape[0]\n",
    "    width = board.shape[1]\n",
    "    \n",
    "    playerColor = board[0, 0] if playerCall == 1 else board[height - 1, width - 1]\n",
    "    found = False\n",
    "    \n",
    "    # Check if the player has a valid action that gains more cells\n",
    "    for cell in frontier:\n",
    "        neighbours = pointsAround(cell)\n",
    "        for neighbour in neighbours:\n",
    "            if (0 <= neighbour[0] < height \n",
    "                and 0 <= neighbour[1] < width \n",
    "                and 0 <= board[neighbour[0], neighbour[1]] < 5):\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "    if found:\n",
    "        return board\n",
    "    \n",
    "    # Award the player the rest of the cells\n",
    "    for x in np.nditer(board, op_flags=['readwrite']):\n",
    "        if 0 <= int(x) < 5:\n",
    "            x[...] = playerColor\n",
    "    return board\n",
    "\n",
    "def getReward(game, player):\n",
    "    # This function calculates the reward of a game\n",
    "    # We reward nothing for a move unless it is a winning move\n",
    "    # Then it gains 1 point. If it is a losing move, it gains -1\n",
    "    if not gameEnded(game):\n",
    "        return 0\n",
    "\n",
    "    height = game.shape[0]\n",
    "    width = game.shape[1]\n",
    "    \n",
    "    player1Color = game[0,0]\n",
    "    player2Color = game[height - 1, width - 1]\n",
    "    player1count = np.count_nonzero(game == player1Color)\n",
    "    player2count = np.count_nonzero(game == player2Color)\n",
    "    \n",
    "    if player == 1:\n",
    "        if player1count > player2count:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        if player2count > player1count:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "def gameEnded(board):\n",
    "    # Simply checks if there are any cells that is not owned \n",
    "    return not np.any(np.logical_and(board >= 0, board < 5))\n",
    "        \n",
    "def pointsAround(point):\n",
    "    y, x = point[0], point[1]\n",
    "    relOddCoords = [\n",
    "        (-1, 0),\n",
    "        (-1, 1),\n",
    "        (0, 1),\n",
    "        (1, 0),\n",
    "        (0, -1),\n",
    "        (-1, -1)\n",
    "    ]\n",
    "\n",
    "    relEvenCoords = [\n",
    "        (-1, 0),\n",
    "        (0, 1),\n",
    "        (1, 1),\n",
    "        (1, 0),\n",
    "        (1, -1),\n",
    "        (0, -1)\n",
    "    ]\n",
    "    \n",
    "    odd = x % 2 == 1\n",
    "    \n",
    "    neighbours = []\n",
    "    for coord in relOddCoords if odd else relEvenCoords:\n",
    "        newY = y + coord[0]\n",
    "        newX = x + coord[1]\n",
    "        neighbours.append([newY, newX])\n",
    "    return neighbours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below is an implementaiton of the Q-Learning-Agent from Russel & Norvig (2010) that maintains a State-Action array $Q$ and uses it to learns the best actions from the reward $r$ of a given state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HexLearner(object):\n",
    "    # This class is an implementation of the Q-Learning-Agent\n",
    "    # from Russel & Norvig (2010) p. 844\n",
    "    \n",
    "    def __init__(self, player, Q = {}, N = {}):\n",
    "        self.Q = Q\n",
    "        self.N = N\n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.r = None\n",
    "        self.player = player\n",
    "        self.actions = [0, 1, 2, 3, 4]\n",
    "    \n",
    "    def getMove(self, state, reward):\n",
    "        if gameEnded(state):\n",
    "            self.Q[getHash(state), None] = reward\n",
    "        if self.s is not None:\n",
    "            self.incrementN()\n",
    "            self.updateQ(state, reward)\n",
    "        self.s = state\n",
    "        self.a = self.argmax(state)\n",
    "        self.r = reward\n",
    "        return self.a\n",
    "    \n",
    "    def initializeQ(self, s, a):\n",
    "        if (s, a) not in self.Q:\n",
    "            self.Q[s, a] = 0\n",
    "    \n",
    "    def initializeN(self, s, a):\n",
    "        if (s, a) not in self.N:\n",
    "            self.N[s, a] = 0\n",
    "    \n",
    "    def argmax(self, state):\n",
    "        vals = []\n",
    "        s = getHash(state)\n",
    "        vals = [self.f(self.Q.get((s, a), 0), self.N.get((s, a), 0)) for a in self.actions]\n",
    "        return self.actions.index(vals.index(max(vals)))\n",
    "            \n",
    "    \n",
    "    def f(self, val, num):\n",
    "        if num < 10:\n",
    "            return 100\n",
    "        return val\n",
    "    \n",
    "    def incrementN(self):\n",
    "        s = getHash(self.s)\n",
    "        a = self.a\n",
    "        self.initializeN(s, a)\n",
    "        self.N[s, a] += 1\n",
    "    \n",
    "    def updateQ(self, sP, rP):\n",
    "        s = getHash(self.s)\n",
    "        sPh = getHash(sP)\n",
    "        a = self.a\n",
    "        self.initializeQ(s, a)\n",
    "        self.Q[s, a] = self.Q[s, a] + self.alpha() * (self.r + max([self.Q.get((sPh, aP), 0) for aP in self.actions]) - self.Q[s, a])\n",
    "        \n",
    "    def alpha(self):\n",
    "        return 60 / (59 + self.N[getHash(self.s), self.a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is ready to learn! The code below lets the agent play against it self and learn along the way. We use a value $\\varepsilon$ to denote the probability of choosing a random action instead of the action supplied by the agent. This is done to force the agent to explore more. $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "NUMTRIALS = 10000\n",
    "EPSILON = 0.1\n",
    "latestQ1 = {}\n",
    "latestN1 = {}\n",
    "latestQ2 = {}\n",
    "latestN2 = {}\n",
    "\n",
    "startTime = time.time()\n",
    "startGame = randomGame(3, 3)\n",
    "\n",
    "for x in range(NUMTRIALS):\n",
    "    agent1 = HexLearner(1, latestQ1, latestN1)\n",
    "    agent2 = HexLearner(2, latestQ2, latestN2)\n",
    "    game = startGame.copy()\n",
    "    won = 2\n",
    "    while not gameEnded(game):\n",
    "        num = np.random.rand()\n",
    "        action = agent1.getMove(game, getReward(game, 1))\n",
    "        if num < EPSILON:\n",
    "            action = np.random.randint(5)\n",
    "        game = makeMove(game, 1, action)\n",
    "\n",
    "        action = agent2.getMove(game, getReward(game, 2))\n",
    "        if num < EPSILON:\n",
    "            action = np.random.randint(5)\n",
    "        game = makeMove(game, 2, action)\n",
    "\n",
    "    agent1.getMove(game, getReward(game, 1))\n",
    "    agent1.getMove(game, getReward(game, 1))\n",
    "    agent2.getMove(game, getReward(game, 2))\n",
    "    latestQ1 = agent1.Q\n",
    "    latestN1 = agent1.N\n",
    "    latestQ2 = agent2.Q\n",
    "    latestN2 = agent2.N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to play against the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 -1  0]\n",
      " [ 1  2  0]\n",
      " [ 4  0  2]\n",
      " [ 4  2 10]]\n",
      "P1: 0\n",
      "[[ 5 -1  0]\n",
      " [ 1  2  0]\n",
      " [ 4  0  2]\n",
      " [ 4  2 10]]\n",
      "2\n",
      "[[ 5 -1  0]\n",
      " [ 1  2  0]\n",
      " [ 4  0 12]\n",
      " [ 4 12 12]]\n",
      "P1: 0\n",
      "[[ 5 -1  0]\n",
      " [ 1  2  0]\n",
      " [ 4  0 12]\n",
      " [ 4 12 12]]\n",
      "4\n",
      "[[ 5 -1  0]\n",
      " [ 1  2  0]\n",
      " [14  0 14]\n",
      " [14 14 14]]\n",
      "P1: 2\n",
      "[[ 7 -1  0]\n",
      " [ 1  7  0]\n",
      " [14  0 14]\n",
      " [14 14 14]]\n",
      "0\n",
      "[[ 7 -1 10]\n",
      " [ 1  7 10]\n",
      " [10 10 10]\n",
      " [10 10 10]]\n",
      "P1: 4\n",
      "[[ 9 -1 10]\n",
      " [ 1  9 10]\n",
      " [10 10 10]\n",
      " [10 10 10]]\n",
      "1\n",
      "[[ 9 -1 11]\n",
      " [11  9 11]\n",
      " [11 11 11]\n",
      " [11 11 11]]\n"
     ]
    }
   ],
   "source": [
    "agent = HexLearner(1, latestQ1, latestN1)\n",
    "game = startGame.copy()\n",
    "print(game)\n",
    "\n",
    "while not gameEnded(game):\n",
    "    action = agent.getMove(game, getReward(game, 1))\n",
    "    if action != None:\n",
    "        print(\"P1: \" + str(action))\n",
    "        game = makeMove(game, 1, action)\n",
    "    print(game)\n",
    "    \n",
    "    game = makeMove(game, 2, int(input()))\n",
    "    print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(latestQ1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent1 = HexLearner(1, latestQ1, latestN1)\n",
    "game = startGame.copy()#randomGame(3, 3)\n",
    "print(game)\n",
    "\n",
    "while not gameEnded(game):\n",
    "    action1 = agent1.getMove(game, getReward(game, 1))\n",
    "    if action1 != None:\n",
    "        print(\"P1: \" + str(action1))\n",
    "        game = makeMove(game, 1, action1)\n",
    "    print(game)\n",
    "    \n",
    "    action2 = agent2.getMove(game, getReward(game, 2))\n",
    "    if action2 != None:\n",
    "        print(\"P2: \" + str(action2))\n",
    "        game = makeMove(game, 2, action2)\n",
    "    print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
